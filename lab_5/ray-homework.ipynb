{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-13T13:39:26.701108Z",
     "start_time": "2025-05-13T13:39:15.144309Z"
    }
   },
   "source": [
    "import ray\n",
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "ray.init(address='ray://localhost:10001', ignore_reinit_error=True)\n",
    "CHUNK_SIZE = 1024\n",
    "REPLICATION_FACTOR = 3"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 15:39:17,768\tINFO client_builder.py:244 -- Passing the following kwargs to ray.init() on the server: ignore_reinit_error, log_to_driver\n",
      "2025-05-13 15:39:25,833\tWARNING utils.py:1569 -- Python patch version mismatch: The cluster was started with:\n",
      "    Ray: 2.44.1\n",
      "    Python: 3.11.11\n",
      "This process on Ray Client was started with:\n",
      "    Ray: 2.44.1\n",
      "    Python: 3.11.5\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T13:39:40.430511Z",
     "start_time": "2025-05-13T13:39:40.418583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@ray.remote\n",
    "class StorageNode:\n",
    "    def __init__(self, node_id):\n",
    "        self.node_id = node_id\n",
    "        self.chunks = {}\n",
    "        self.alive = True\n",
    "\n",
    "    def store_chunk(self, artifact_id, chunk_idx, data):\n",
    "        if not self.alive:\n",
    "            raise RuntimeError(f\"Node {self.node_id} is down\")\n",
    "        self.chunks.setdefault(artifact_id, {})[chunk_idx] = data\n",
    "        return True\n",
    "\n",
    "    def get_chunk(self, artifact_id, chunk_idx):\n",
    "        return self.chunks.get(artifact_id, {}).get(chunk_idx)\n",
    "\n",
    "    def delete_artifact(self, artifact_id):\n",
    "        self.chunks.pop(artifact_id, None)\n",
    "        return True\n",
    "\n",
    "    def list_chunks(self):\n",
    "        return {aid: list(m.keys()) for aid, m in self.chunks.items()}\n",
    "\n",
    "    def set_status(self, alive: bool):\n",
    "        self.alive = alive\n",
    "        return self.alive\n",
    "\n",
    "    def is_alive(self):\n",
    "        return self.alive\n",
    "\n",
    "\n"
   ],
   "id": "cf124eb484a76760",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T13:39:43.043159Z",
     "start_time": "2025-05-13T13:39:43.019811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@ray.remote\n",
    "class NameNode:\n",
    "    def __init__(self):\n",
    "        self.metadata = {}\n",
    "        self.locations = {}\n",
    "        self.nodes = {}\n",
    "\n",
    "    def register_node(self, node_id, actor_handle):\n",
    "        self.nodes[node_id] = actor_handle\n",
    "        return True\n",
    "\n",
    "    def unregister_node(self, node_id):\n",
    "        self.nodes.pop(node_id, None)\n",
    "        return True\n",
    "\n",
    "    def put_artifact(self, artifact_id, name, data):\n",
    "        chunks = [data[i:i+CHUNK_SIZE] for i in range(0, len(data), CHUNK_SIZE)]\n",
    "        self.metadata[artifact_id] = {'name': name, 'chunks': len(chunks)}\n",
    "        self.locations[artifact_id] = {}\n",
    "        node_ids = list(self.nodes.keys())\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            replicas = [node_ids[(idx + r) % len(node_ids)] for r in range(REPLICATION_FACTOR)]\n",
    "            self.locations[artifact_id][idx] = replicas\n",
    "            for n_id in replicas:\n",
    "                self.nodes[n_id].store_chunk.remote(artifact_id, idx, chunk)\n",
    "        return True\n",
    "\n",
    "    def get_artifact(self, artifact_id):\n",
    "        if artifact_id not in self.metadata:\n",
    "            raise KeyError(\"Artifact not found\")\n",
    "        total = self.metadata[artifact_id]['chunks']\n",
    "        result = []\n",
    "        for idx in range(total):\n",
    "            for n_id in self.locations[artifact_id][idx]:\n",
    "                node = self.nodes[n_id]\n",
    "                if ray.get(node.is_alive.remote()):\n",
    "                    data = ray.get(node.get_chunk.remote(artifact_id, idx))\n",
    "                    result.append(data)\n",
    "                    break\n",
    "            else:\n",
    "                raise RuntimeError(f\"No alive replica for chunk {idx}\")\n",
    "        return self.metadata[artifact_id]['name'], ''.join(result)\n",
    "\n",
    "    def update_artifact(self, artifact_id, new_data):\n",
    "        name = self.metadata[artifact_id]['name']\n",
    "        self.delete_artifact(artifact_id)\n",
    "        return self.put_artifact(artifact_id, name, new_data)\n",
    "\n",
    "    def delete_artifact(self, artifact_id):\n",
    "        for idx, nodes in self.locations.get(artifact_id, {}).items():\n",
    "            for n_id in nodes:\n",
    "                if n_id in self.nodes:\n",
    "                    self.nodes[n_id].delete_artifact.remote(artifact_id)\n",
    "        self.metadata.pop(artifact_id, None)\n",
    "        self.locations.pop(artifact_id, None)\n",
    "        return True\n",
    "\n",
    "    def list_metadata(self):\n",
    "        return self.metadata\n",
    "\n",
    "    def list_locations(self):\n",
    "        return self.locations\n"
   ],
   "id": "e4fcf6d43393796",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T13:45:52.703643Z",
     "start_time": "2025-05-13T13:45:47.373411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "name_node = NameNode.remote()\n",
    "nodes = {}\n",
    "for i in range(5):\n",
    "    nid = f\"node-{i}\"\n",
    "    node = StorageNode.remote(nid)\n",
    "    nodes[nid] = node\n",
    "    ray.get(name_node.register_node.remote(nid, node))\n",
    "\n",
    "node_with_options = StorageNode.options(name=\"node-special\", num_cpus=0.3).remote(\"node-special\")\n",
    "nodes['node-special'] = node_with_options\n",
    "ray.get(name_node.register_node.remote(\"node-special\", node_with_options))\n",
    "\n",
    "# PUT\n",
    "long_data = 'A' * 5000\n",
    "ray.get(name_node.put_artifact.remote('art1', 'TestArtifact', long_data))\n",
    "print('Metadata after PUT:', ray.get(name_node.list_metadata.remote()))\n",
    "print('Locations after PUT:', ray.get(name_node.list_locations.remote()))\n",
    "\n",
    "# GET\n",
    "name, content = ray.get(name_node.get_artifact.remote('art1'))\n",
    "print(f\"GET: {name}, length={len(content)}\")\n",
    "\n",
    "# UPDATE\n",
    "new_data = 'B' * 3000\n",
    "ray.get(name_node.update_artifact.remote('art1', new_data))\n",
    "print('Metadata after UPDATE:', ray.get(name_node.list_metadata.remote()))\n",
    "\n",
    "# DELETE\n",
    "ray.get(name_node.delete_artifact.remote('art1'))\n",
    "print('Metadata after DELETE:', ray.get(name_node.list_metadata.remote()))\n",
    "\n",
    "# Failover\n",
    "ray.get(nodes['node-2'].set_status.remote(False))\n",
    "ray.get(name_node.put_artifact.remote('art2', 'Artifact2', 'C'*2048))\n",
    "print('Locations with failure:', ray.get(name_node.list_locations.remote()))\n",
    "name2, content2 = ray.get(name_node.get_artifact.remote('art2'))\n",
    "print(f\"GET after failure: {name2}, length={len(content2)}\")\n",
    "\n"
   ],
   "id": "1143c1b28428f060",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata after PUT: {'art1': {'name': 'TestArtifact', 'chunks': 5}}\n",
      "Locations after PUT: {'art1': {0: ['node-0', 'node-1', 'node-2'], 1: ['node-1', 'node-2', 'node-3'], 2: ['node-2', 'node-3', 'node-4'], 3: ['node-3', 'node-4', 'node-special'], 4: ['node-4', 'node-special', 'node-0']}}\n"
     ]
    },
    {
     "ename": "RayTaskError(OutOfMemoryError)",
     "evalue": "\u001B[36mray::NameNode.get_artifact()\u001B[39m (pid=687, ip=172.63.0.3, actor_id=6b47d04db87bc42b32d4b85901000000, repr=<__main__.NameNode object at 0x7f4a28db8b90>)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dominik\\ComputerScienceStudiesAGH\\term_VI\\DISTRIBUTED_SYSTEMS\\lab_5\\lab5-ray\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 463, in _resume_span\n  File \"C:\\Users\\Dominik\\AppData\\Local\\Temp\\ipykernel_26056\\3690584755.py\", line 36, in get_artifact\n           ^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\nMemory on the node (IP: 172.63.0.2, ID: ea270dbab81b9c7f5da6ad7ba916018d9fe795c223fe57dffc3f7e98) where the task (task ID: ffffffffffffffffe84026cf006ea2204420f3ba01000000, name=StorageNode.__init__, pid=1310, memory used=0.05GB) was running was 1.91GB / 2.00GB (0.956745), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c62b83f78a5b92cb95c480e2a14a746f6b5995fec5b9e08ee8fd286b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.63.0.2`. To see the logs of the worker, use `ray logs worker-c62b83f78a5b92cb95c480e2a14a746f6b5995fec5b9e08ee8fd286b*out -ip 172.63.0.2. Top 10 memory users:\nPID\tMEM(GB)\tCOMMAND\n82\t0.38\t/home/ray/anaconda3/bin/python3.11 /home/ray/anaconda3/lib/python3.11/site-packages/ray/dashboard/da...\n389\t0.32\t/home/ray/anaconda3/bin/python3.11 -m ray.util.client.server --address=172.63.0.2:6379 --host=0.0.0....\n81\t0.27\t/home/ray/anaconda3/bin/python3.11 -m ray.util.client.server --address=172.63.0.2:6379 --host=0.0.0....\n22\t0.20\t/home/ray/anaconda3/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/...\n218\t0.19\t/home/ray/anaconda3/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name...\n219\t0.14\t/home/ray/anaconda3/bin/python3.11 -u /home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/...\n80\t0.11\t/home/ray/anaconda3/bin/python3.11 -u /home/ray/anaconda3/lib/python3.11/site-packages/ray/autoscale...\n294\t0.08\t/home/ray/anaconda3/bin/python3.11 -u /home/ray/anaconda3/lib/python3.11/site-packages/ray/dashboard...\n1310\t0.05\tray::StorageNode.__init__\n296\t0.05\t/home/ray/anaconda3/bin/python3.11 -u /home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/...\nRefer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRayTaskError(OutOfMemoryError)\u001B[39m            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 20\u001B[39m\n\u001B[32m     17\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m'\u001B[39m\u001B[33mLocations after PUT:\u001B[39m\u001B[33m'\u001B[39m, ray.get(name_node.list_locations.remote()))\n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m# GET\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m20\u001B[39m name, content = \u001B[43mray\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_node\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_artifact\u001B[49m\u001B[43m.\u001B[49m\u001B[43mremote\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mart1\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     21\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mGET: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, length=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(content)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     23\u001B[39m \u001B[38;5;66;03m# UPDATE\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\ComputerScienceStudiesAGH\\term_VI\\DISTRIBUTED_SYSTEMS\\lab_5\\lab5-ray\\Lib\\site-packages\\ray\\_private\\auto_init_hook.py:21\u001B[39m, in \u001B[36mwrap_auto_init.<locals>.auto_init_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     18\u001B[39m \u001B[38;5;129m@wraps\u001B[39m(fn)\n\u001B[32m     19\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mauto_init_wrapper\u001B[39m(*args, **kwargs):\n\u001B[32m     20\u001B[39m     auto_init_ray()\n\u001B[32m---> \u001B[39m\u001B[32m21\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\ComputerScienceStudiesAGH\\term_VI\\DISTRIBUTED_SYSTEMS\\lab_5\\lab5-ray\\Lib\\site-packages\\ray\\_private\\client_mode_hook.py:102\u001B[39m, in \u001B[36mclient_mode_hook.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     98\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m client_mode_should_convert():\n\u001B[32m     99\u001B[39m     \u001B[38;5;66;03m# Legacy code\u001B[39;00m\n\u001B[32m    100\u001B[39m     \u001B[38;5;66;03m# we only convert init function if RAY_CLIENT_MODE=1\u001B[39;00m\n\u001B[32m    101\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m func.\u001B[34m__name__\u001B[39m != \u001B[33m\"\u001B[39m\u001B[33minit\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m is_client_mode_enabled_by_default:\n\u001B[32m--> \u001B[39m\u001B[32m102\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mray\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__name__\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    103\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m func(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\ComputerScienceStudiesAGH\\term_VI\\DISTRIBUTED_SYSTEMS\\lab_5\\lab5-ray\\Lib\\site-packages\\ray\\util\\client\\api.py:42\u001B[39m, in \u001B[36m_ClientAPI.get\u001B[39m\u001B[34m(self, vals, timeout)\u001B[39m\n\u001B[32m     35\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget\u001B[39m(\u001B[38;5;28mself\u001B[39m, vals, *, timeout=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m     36\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"get is the hook stub passed on to replace `ray.get`\u001B[39;00m\n\u001B[32m     37\u001B[39m \n\u001B[32m     38\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m     39\u001B[39m \u001B[33;03m        vals: [Client]ObjectRef or list of these refs to retrieve.\u001B[39;00m\n\u001B[32m     40\u001B[39m \u001B[33;03m        timeout: Optional timeout in milliseconds\u001B[39;00m\n\u001B[32m     41\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mworker\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvals\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\ComputerScienceStudiesAGH\\term_VI\\DISTRIBUTED_SYSTEMS\\lab_5\\lab5-ray\\Lib\\site-packages\\ray\\util\\client\\worker.py:433\u001B[39m, in \u001B[36mWorker.get\u001B[39m\u001B[34m(self, vals, timeout)\u001B[39m\n\u001B[32m    431\u001B[39m     op_timeout = max_blocking_operation_time\n\u001B[32m    432\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m433\u001B[39m     res = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get\u001B[49m\u001B[43m(\u001B[49m\u001B[43mto_get\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_timeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    434\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m    435\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m GetTimeoutError:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\ComputerScienceStudiesAGH\\term_VI\\DISTRIBUTED_SYSTEMS\\lab_5\\lab5-ray\\Lib\\site-packages\\ray\\util\\client\\worker.py:461\u001B[39m, in \u001B[36mWorker._get\u001B[39m\u001B[34m(self, ref, timeout)\u001B[39m\n\u001B[32m    459\u001B[39m         logger.exception(\u001B[33m\"\u001B[39m\u001B[33mFailed to deserialize \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m\"\u001B[39m.format(chunk.error))\n\u001B[32m    460\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m461\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m err\n\u001B[32m    462\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chunk.total_size > OBJECT_TRANSFER_WARNING_SIZE \u001B[38;5;129;01mand\u001B[39;00m log_once(\n\u001B[32m    463\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mclient_object_transfer_size_warning\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    464\u001B[39m ):\n\u001B[32m    465\u001B[39m     size_gb = chunk.total_size / \u001B[32m2\u001B[39m**\u001B[32m30\u001B[39m\n",
      "\u001B[31mRayTaskError(OutOfMemoryError)\u001B[39m: \u001B[36mray::NameNode.get_artifact()\u001B[39m (pid=687, ip=172.63.0.3, actor_id=6b47d04db87bc42b32d4b85901000000, repr=<__main__.NameNode object at 0x7f4a28db8b90>)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dominik\\ComputerScienceStudiesAGH\\term_VI\\DISTRIBUTED_SYSTEMS\\lab_5\\lab5-ray\\Lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 463, in _resume_span\n  File \"C:\\Users\\Dominik\\AppData\\Local\\Temp\\ipykernel_26056\\3690584755.py\", line 36, in get_artifact\n           ^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\nMemory on the node (IP: 172.63.0.2, ID: ea270dbab81b9c7f5da6ad7ba916018d9fe795c223fe57dffc3f7e98) where the task (task ID: ffffffffffffffffe84026cf006ea2204420f3ba01000000, name=StorageNode.__init__, pid=1310, memory used=0.05GB) was running was 1.91GB / 2.00GB (0.956745), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c62b83f78a5b92cb95c480e2a14a746f6b5995fec5b9e08ee8fd286b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.63.0.2`. To see the logs of the worker, use `ray logs worker-c62b83f78a5b92cb95c480e2a14a746f6b5995fec5b9e08ee8fd286b*out -ip 172.63.0.2. Top 10 memory users:\nPID\tMEM(GB)\tCOMMAND\n82\t0.38\t/home/ray/anaconda3/bin/python3.11 /home/ray/anaconda3/lib/python3.11/site-packages/ray/dashboard/da...\n389\t0.32\t/home/ray/anaconda3/bin/python3.11 -m ray.util.client.server --address=172.63.0.2:6379 --host=0.0.0....\n81\t0.27\t/home/ray/anaconda3/bin/python3.11 -m ray.util.client.server --address=172.63.0.2:6379 --host=0.0.0....\n22\t0.20\t/home/ray/anaconda3/lib/python3.11/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/...\n218\t0.19\t/home/ray/anaconda3/lib/python3.11/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name...\n219\t0.14\t/home/ray/anaconda3/bin/python3.11 -u /home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/...\n80\t0.11\t/home/ray/anaconda3/bin/python3.11 -u /home/ray/anaconda3/lib/python3.11/site-packages/ray/autoscale...\n294\t0.08\t/home/ray/anaconda3/bin/python3.11 -u /home/ray/anaconda3/lib/python3.11/site-packages/ray/dashboard...\n1310\t0.05\tray::StorageNode.__init__\n296\t0.05\t/home/ray/anaconda3/bin/python3.11 -u /home/ray/anaconda3/lib/python3.11/site-packages/ray/_private/...\nRefer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero."
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
